{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_features(df, number_features):\n",
    "    \n",
    "    columns = list(df.columns)\n",
    "    chosen_columns = []\n",
    "    \n",
    "    if number_features == 92:\n",
    "        for column in columns:\n",
    "            if column.find('L5') != -1 or column.find('L3') != -1 or column.find('L1') != -1 or column.find('L=0.1') != -1:\n",
    "                chosen_columns.append(column)\n",
    "        df = pd.DataFrame(df, columns = chosen_columns)\n",
    "        return df\n",
    "    elif number_features == 69:\n",
    "        for column in columns:\n",
    "            if column.find('L5') != -1 or column.find('L3') != -1 or column.find('L1') != -1:\n",
    "                chosen_columns.append(column)\n",
    "        df = pd.DataFrame(df, columns = chosen_columns)\n",
    "        return df\n",
    "    elif number_features == 46:\n",
    "        for column in columns:\n",
    "            if column.find('L5') != -1 or column.find('L3') != -1:\n",
    "                chosen_columns.append(column)\n",
    "        df = pd.DataFrame(df, columns = chosen_columns)\n",
    "        return df\n",
    "    elif number_features == 23:\n",
    "        for column in columns:\n",
    "            if column.find('L5') != -1:\n",
    "                chosen_columns.append(column)\n",
    "        df = pd.DataFrame(df, columns = chosen_columns)\n",
    "        return df\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path, number_features):\n",
    "    \n",
    "    all_files = glob.glob(os.path.join(path , '*.csv'))\n",
    "\n",
    "    files_list = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file, index_col = None, encoding = 'utf-8', sep = ',', low_memory = False)\n",
    "        files_list.append(df)\n",
    "        \n",
    "    df = pd.concat(files_list, axis = 0, ignore_index = True)\n",
    "    df = get_number_features(df, number_features)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a54a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_botiot(path):\n",
    "    \n",
    "    all_files = glob.glob(os.path.join(path , '*.csv'))\n",
    "\n",
    "    files_list = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file, index_col = None, encoding = 'utf-8', sep = ',', low_memory = False)\n",
    "        files_list.append(df)\n",
    "        \n",
    "    df = pd.concat(files_list, axis = 0, ignore_index = True)\n",
    "    \n",
    "    # Drop irrelevant information\n",
    "    df = df.drop(columns=['pkSeqID', 'stime', 'flgs', 'flgs_number', 'saddr', 'sport', 'daddr', 'dport', 'subcategory', 'category'])\n",
    "    \n",
    "    # Categorical to numerical\n",
    "    df['proto'] = df['proto'].map({'tcp': 1, 'arp': 2, 'udp': 3, 'icmp': 4, 'ipv6-icmp': 5})\n",
    "    df['state'] = df['state'].map({'REQ': 1, 'RST': 2, 'ACC': 3, 'CON': 4, 'INT': 5, 'URP': 6, 'FIN': 7, 'NRS': 8, 'ECO': 9, 'TST': 10, 'MAS': 11})\n",
    "    \n",
    "    df_benign = df\n",
    "    df_attack = df\n",
    "    \n",
    "    # Get only benign data\n",
    "    df_benign = df_benign.query('attack == 0')\n",
    "    df_benign = df_benign.drop(columns=['attack'])\n",
    "    \n",
    "    # Get only the attacks\n",
    "    df_attack = df_attack.query('attack == 1')\n",
    "    df_attack = df_attack.drop(columns=['attack'])\n",
    "        \n",
    "    return df_benign, df_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9edc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparameterization trick\n",
    "# Sample the normally distributed z - mean + sigma * epsilon. The epsilon ensures the continuity of latent space and helps\n",
    "# the network to keep correcting its parameters through backpropagation\n",
    "\n",
    "def reparametrization(args):\n",
    "    \n",
    "    z_mean, z_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape = (batch, dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407708d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get error term\n",
    "# Calculates the error between the original vector and the predicted one\n",
    "\n",
    "def get_error_term(v1, v2, _rmse = True):\n",
    "    \n",
    "    if _rmse:\n",
    "        return np.sqrt(np.mean((v1 - v2) ** 2, axis = 1))\n",
    "    \n",
    "    return np.mean(abs(v1 - v2), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Model\n",
    "# The encoder learns a function that takes an input array of size n and can generate two vectors that represents the\n",
    "# parameters (mean and variance) of a distribution from which the latent vector is sampled.\n",
    "\n",
    "# encoder(input_vector[]) => latent_v_mu[], latent_v_lvar[]\n",
    "# So that - latent_v[0] ~  N(latent_v_mu[0], latent_v_lvar[0])\n",
    "# and latent_v[1] ~  N(latent_v_mu[1], latent_v_lvar[1])\n",
    "\n",
    "def vae_encoder(input_shape, intermediate_dim, latent_dim, reparametrization):\n",
    "    \n",
    "    inputs = Input(shape = input_shape, name = 'encoder_input')\n",
    "    x = Dense(intermediate_dim, activation = 'relu')(inputs)\n",
    "\n",
    "    z_mean = Dense(latent_dim, name = 'z_mean')(x)\n",
    "    z_var = Dense(latent_dim, name = 'z_var')(x)\n",
    "    z = Lambda(reparametrization, output_shape = (latent_dim,), name = 'z')([z_mean, z_var])\n",
    "\n",
    "    encoder = Model(inputs, z, name = 'encoder')\n",
    "    return inputs, encoder, z_var, z_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9faffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder model\n",
    "# Transforms the latent feature space composed by distributions of mean and variance back to the original input vector\n",
    "\n",
    "def vae_decoder(intermediate_dim, latent_dim, original_dim):\n",
    "    \n",
    "    latent_inputs = Input(shape = (latent_dim,), name = 'z_sampling')\n",
    "    x = Dense(intermediate_dim, activation = 'relu')(latent_inputs)\n",
    "    outputs = Dense(original_dim, activation = 'sigmoid')(x)\n",
    "\n",
    "    # Instantiate the decoder model\n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name = 'decoder')\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43306534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train, inputs, outputs, vae_loss, learning_rate, epochs, batch_size):\n",
    "    \n",
    "    # Create model\n",
    "    adam_opt = Adam(learning_rate = learning_rate, clipvalue = 0.5)\n",
    "    model = Model(inputs, outputs, name = 'vae_mlp')\n",
    "    model.compile(optimizer = adam_opt, loss = vae_loss)\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(X_train, X_train, shuffle = True, epochs = epochs, batch_size = batch_size, verbose = 1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, inputs, outputs, vae_loss, learning_rate, epochs, batch_size):\n",
    "    \n",
    "    train_start = time.time()\n",
    "\n",
    "    model = fit_model(X_train, inputs, outputs, vae_loss, learning_rate, epochs, batch_size)\n",
    "\n",
    "    train_end = time.time()\n",
    "    train_time = train_end - train_start\n",
    "    print(\"Training time:\", train_time)\n",
    "    \n",
    "    return model, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, model):\n",
    "    \n",
    "    test_start = time.time()\n",
    "\n",
    "    X_pred = model.predict(X_test)\n",
    "\n",
    "    test_end = time.time()\n",
    "    test_time = test_end - test_start\n",
    "    print(\"Testing time:\", test_time)\n",
    "    \n",
    "    return X_pred, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the anomaly threshold based on the error termo between the predicted train set and the real one\n",
    "\n",
    "def get_anomaly_threshold(X_train, model):\n",
    "    \n",
    "    X_pred = model.predict(X_train)\n",
    "    error_vector = get_error_term(X_pred, X_train, _rmse = False)\n",
    "    anomaly_threshold = np.quantile(error_vector, 0.99)\n",
    "    \n",
    "    return anomaly_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8190e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the error of the vector is higher than the defined threshold it detects an attack, generating the prediction vector\n",
    "\n",
    "def get_prediction(Y_test, X_pred, X_test, anomaly_threshold, model):\n",
    "    \n",
    "    error_vector = get_error_term(X_pred, X_test, _rmse = False)\n",
    "    Y_pred = (error_vector > anomaly_threshold)\n",
    "    Y_pred = Y_pred.astype(int)\n",
    "    Y_test = Y_test.astype(int)\n",
    "        \n",
    "    return Y_test, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292923e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(Y_test, Y_pred):\n",
    "    \n",
    "    acc = accuracy_score(Y_test, Y_pred) \n",
    "    f1 = f1_score(Y_test, Y_pred)\n",
    "    pre = precision_score(Y_test, Y_pred)\n",
    "    rec = recall_score(Y_test, Y_pred)\n",
    "    \n",
    "    return acc, f1, pre, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aee939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(number_features,\n",
    "                  learning_rate,\n",
    "                  epochs,\n",
    "                  batch_size,\n",
    "                  anomaly_threshold,\n",
    "                  X_train,\n",
    "                  X_test,\n",
    "                  opt_time,\n",
    "                  train_time,\n",
    "                  test_time,\n",
    "                  acc,\n",
    "                  f1,\n",
    "                  pre,\n",
    "                  rec,\n",
    "                  Y_test,\n",
    "                  Y_pred,\n",
    "                  path):\n",
    "    \n",
    "    stdout_obj = sys.stdout\n",
    "    sys.stdout = open(path, \"a\")\n",
    "\n",
    "    print(\"==== Experiment with \" + str(number_features) + \" features ====\")\n",
    "    print(\"Learning rate:\" + str(learning_rate) + \" - Epochs:\" + str(epochs) + \" - Batch size:\" + str(batch_size) + \" - Anomaly threshold:\" + str(anomaly_threshold))\n",
    "    print(\"Training size:\" + str(len(X_train)) + \" - Testing size:\" + str(len(X_test)))\n",
    "    print(\"Optimization time:\" + str(opt_time) + \" - Training time:\" + str(train_time) + \" - Testing time:\" + str(test_time))\n",
    "    print(\"Accuracy:\" + str(acc))\n",
    "    print(\"F1-score:\" + str(f1))\n",
    "    print(\"Precision:\" + str(pre))\n",
    "    print(\"Recall:\" + str(rec))\n",
    "    print(classification_report(Y_test, Y_pred, digits = 5))\n",
    "    print(\"=================================================================\")\n",
    "\n",
    "    sys.stdout = stdout_obj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
